{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing an Physics-informed neural network for the 1D Schrodinger equation using the PINN framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Including necessary libaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PINNFramework.PINN import Interface\n",
    "from torch.autograd import grad\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import scipy.io\n",
    "from pyDOE import lhs\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Underlying PDE\n",
    "$f:=i h_{t}+0.5 h_{x x}+|h|^{2} h$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing needed functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SchrodingerPINN(Interface):\n",
    "    def __init__(self, model, input_d, output_d, lb, ub):\n",
    "        super().__init__(model,input_d,output_d)\n",
    "        self.lb = lb\n",
    "        self.ub = ub\n",
    "        \n",
    "    def pde(self, x, u, derivatives):\n",
    "        u_xx = derivatives[:,0]\n",
    "        v_xx = derivatives[:,1]\n",
    "        _u = u[:,0]\n",
    "        _v = u[:,1]\n",
    "        real_part = - 0.5 * v_xx - (_u**2 - _v**2)*_v\n",
    "        imaginary_part= 0.5 * u_xx + (_u**2 + _v**2)*_u \n",
    "        result = torch.stack([real_part,imaginary_part],1)\n",
    "        return result\n",
    "        \n",
    "    def derivatives(self, u, x):\n",
    "        grads= torch.ones(x.shape[0])\n",
    "        pred_u = u[:,0]\n",
    "        pred_v = u[:,1]\n",
    "        J_u = grad(pred_u, x, create_graph=True, grad_outputs=grads)[0]\n",
    "        J_v = grad(pred_v, x, create_graph=True, grad_outputs=grads)[0]\n",
    "        \n",
    "        #calculate first order derivatives\n",
    "        u_x = J_u[:,0]\n",
    "        u_t = J_u[:,1]\n",
    "\n",
    "        v_x = J_v[:,0]\n",
    "        v_t = J_v[:,1]\n",
    "        \n",
    "        # calculate second order derivatives\n",
    "        J_u_x = grad(u_x, x, create_graph=True, grad_outputs=grads)[0]\n",
    "        J_v_x = grad(v_x, x, create_graph=True, grad_outputs=grads)[0]\n",
    "\n",
    "        u_xx = J_u_x[:,0]\n",
    "        v_xx = J_v_x[:,0]\n",
    "        pred_derivatives = torch.stack([u_xx,v_xx,u_t,v_t],1)\n",
    "        return pred_derivatives\n",
    "    \n",
    "        \n",
    "    def input_normalization(self,x):\n",
    "        \"\"\"\n",
    "        Implementation of min-max scaling in range of [-1,1]\n",
    "        \"\"\"\n",
    "        return 2.0 * (x - self.lb) / (self.ub - self.lb) - 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Creating a model with the sequential API from torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinn_model = nn.Sequential(\n",
    "          nn.Linear(2,100),\n",
    "          nn.Tanh(),\n",
    "          nn.Linear(100,100),\n",
    "          nn.Tanh(),\n",
    "          nn.Linear(100,2)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = torch.tensor([-5.0, 0.0])\n",
    "ub = torch.tensor([[5.0, np.pi / 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SchrodingerPINN(model = pinn_model, input_d = 2, output_d = 2, lb = lb, ub= ub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing forward function of the model (testing normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_x = torch.randn(100,2)\n",
    "sample_pred = model(sample_x)\n",
    "sample_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data (implement your dataset here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = 0.0\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Doman bounds\n",
    "lb = np.array([-5.0, 0.0])  # lower bound consists of [lower bound of x, lower bound of t]\n",
    "ub = np.array([5.0, np.pi / 2])  # upper bound follows from lower bound\n",
    "\n",
    "# defines the sizes of the neural network\n",
    "N0 = 50\n",
    "N_b = 50\n",
    "N_f = 20000\n",
    "\n",
    "data = scipy.io.loadmat('NLS.mat')\n",
    "\n",
    "\n",
    "t = data['tt'].flatten()[:, None]  # get timestamps\n",
    "x = data['x'].flatten()[:, None]  # get x positions\n",
    "Exact = data['uu']\n",
    "# definie labels\n",
    "Exact_u = np.real(Exact)\n",
    "Exact_v = np.imag(Exact)\n",
    "Exact_h = np.sqrt(Exact_u ** 2 + Exact_v ** 2)\n",
    "\n",
    "X, T = np.meshgrid(x, t)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:, None], T.flatten()[:, None]))  # concats the arrays\n",
    "u_star = Exact_u.T.flatten()[:, None]  #\n",
    "v_star = Exact_v.T.flatten()[:, None]\n",
    "h_star = Exact_h.T.flatten()[:, None]\n",
    "\n",
    "###########################\n",
    "\n",
    "idx_x = np.random.choice(x.shape[0], N0, replace=False)\n",
    "idx_x = np.sort(idx_x)\n",
    "\n",
    "x0 = x[idx_x, :]\n",
    "u0 = Exact_u[idx_x, 0:1]\n",
    "v0 = Exact_v[idx_x, 0:1]\n",
    "\n",
    "idx_t = np.random.choice(t.shape[0], N_b, replace=False)\n",
    "idx_t = np.sort(idx_t)\n",
    "tb = t[idx_t, :]\n",
    "\n",
    "x_f = lb + (ub - lb) * lhs(2, N_f) # determine sampling points \n",
    "t0 = torch.zeros([x0.shape[0],1])\n",
    "\n",
    "X_lb = np.concatenate((0 * tb + lb[0], tb), 1)  # (lb[0], tb)\n",
    "X_ub = np.concatenate((0 * tb + ub[0], tb), 1)  # (ub[0], tb)\n",
    "\n",
    "x_b = np.vstack((X_lb,X_ub)) # [x,t]\n",
    "x_0 = np.concatenate([x0,t0],1)\n",
    "u_b = np.zeros(x_b.shape)\n",
    "u_0 = np.concatenate([u0,v0],1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create input_data dictionary and transfer data to torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = {\"x_0\": torch.tensor(x_0).float(), \"x_b\": torch.tensor(x_b).float(), \"x_f\":torch.tensor(x_f).float()}\n",
    "u_0 = torch.tensor(u_0).float()\n",
    "u_b = torch.tensor(u_b).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Optimizer for training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(),lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training_loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0038, grad_fn=<MseLossBackward>)\n",
      "tensor(13.2880, grad_fn=<MseLossBackward>)\n",
      "tensor(3.9889, grad_fn=<MseLossBackward>)\n",
      "Epoch 1 Loss 17.2807922363:\n",
      "tensor(0.0026, grad_fn=<MseLossBackward>)\n",
      "tensor(12.5082, grad_fn=<MseLossBackward>)\n",
      "tensor(3.7640, grad_fn=<MseLossBackward>)\n",
      "Epoch 2 Loss 16.2747497559:\n",
      "tensor(0.0023, grad_fn=<MseLossBackward>)\n",
      "tensor(11.7549, grad_fn=<MseLossBackward>)\n",
      "tensor(3.5471, grad_fn=<MseLossBackward>)\n",
      "Epoch 3 Loss 15.3043012619:\n",
      "tensor(0.0034, grad_fn=<MseLossBackward>)\n",
      "tensor(11.0258, grad_fn=<MseLossBackward>)\n",
      "tensor(3.3372, grad_fn=<MseLossBackward>)\n",
      "Epoch 4 Loss 14.3664388657:\n",
      "tensor(0.0077, grad_fn=<MseLossBackward>)\n",
      "tensor(10.3197, grad_fn=<MseLossBackward>)\n",
      "tensor(3.1334, grad_fn=<MseLossBackward>)\n",
      "Epoch 5 Loss 13.4608135223:\n",
      "tensor(0.0197, grad_fn=<MseLossBackward>)\n",
      "tensor(9.6360, grad_fn=<MseLossBackward>)\n",
      "tensor(2.9349, grad_fn=<MseLossBackward>)\n",
      "Epoch 6 Loss 12.5906772614:\n",
      "tensor(0.0504, grad_fn=<MseLossBackward>)\n",
      "tensor(8.9745, grad_fn=<MseLossBackward>)\n",
      "tensor(2.7410, grad_fn=<MseLossBackward>)\n",
      "Epoch 7 Loss 11.7659301758:\n",
      "tensor(0.1204, grad_fn=<MseLossBackward>)\n",
      "tensor(8.3356, grad_fn=<MseLossBackward>)\n",
      "tensor(2.5513, grad_fn=<MseLossBackward>)\n",
      "Epoch 8 Loss 11.0073299408:\n",
      "tensor(0.2626, grad_fn=<MseLossBackward>)\n",
      "tensor(7.7219, grad_fn=<MseLossBackward>)\n",
      "tensor(2.3657, grad_fn=<MseLossBackward>)\n",
      "Epoch 9 Loss 10.3502140045:\n",
      "tensor(0.5203, grad_fn=<MseLossBackward>)\n",
      "tensor(7.1390, grad_fn=<MseLossBackward>)\n",
      "tensor(2.1851, grad_fn=<MseLossBackward>)\n",
      "Epoch 10 Loss 9.8443059921:\n",
      "tensor(0.9332, grad_fn=<MseLossBackward>)\n",
      "tensor(6.5981, grad_fn=<MseLossBackward>)\n",
      "tensor(2.0122, grad_fn=<MseLossBackward>)\n",
      "Epoch 11 Loss 9.5435142517:\n",
      "tensor(1.5070, grad_fn=<MseLossBackward>)\n",
      "tensor(6.1192, grad_fn=<MseLossBackward>)\n",
      "tensor(1.8535, grad_fn=<MseLossBackward>)\n",
      "Epoch 12 Loss 9.4797859192:\n",
      "tensor(2.1764, grad_fn=<MseLossBackward>)\n",
      "tensor(5.7312, grad_fn=<MseLossBackward>)\n",
      "tensor(1.7194, grad_fn=<MseLossBackward>)\n",
      "Epoch 13 Loss 9.6269216537:\n",
      "tensor(2.8000, grad_fn=<MseLossBackward>)\n",
      "tensor(5.4626, grad_fn=<MseLossBackward>)\n",
      "tensor(1.6212, grad_fn=<MseLossBackward>)\n",
      "Epoch 14 Loss 9.8838129044:\n",
      "tensor(3.2096, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3253, grad_fn=<MseLossBackward>)\n",
      "tensor(1.5651, grad_fn=<MseLossBackward>)\n",
      "Epoch 15 Loss 10.0999269485:\n",
      "tensor(3.2980, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3062, grad_fn=<MseLossBackward>)\n",
      "tensor(1.5478, grad_fn=<MseLossBackward>)\n",
      "Epoch 16 Loss 10.1519966125:\n",
      "tensor(3.0874, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3779, grad_fn=<MseLossBackward>)\n",
      "tensor(1.5607, grad_fn=<MseLossBackward>)\n",
      "Epoch 17 Loss 10.0259857178:\n",
      "tensor(2.6926, grad_fn=<MseLossBackward>)\n",
      "tensor(5.5130, grad_fn=<MseLossBackward>)\n",
      "tensor(1.5942, grad_fn=<MseLossBackward>)\n",
      "Epoch 18 Loss 9.7997188568:\n",
      "tensor(2.2377, grad_fn=<MseLossBackward>)\n",
      "tensor(5.6879, grad_fn=<MseLossBackward>)\n",
      "tensor(1.6405, grad_fn=<MseLossBackward>)\n",
      "Epoch 19 Loss 9.5660972595:\n",
      "tensor(1.8093, grad_fn=<MseLossBackward>)\n",
      "tensor(5.8831, grad_fn=<MseLossBackward>)\n",
      "tensor(1.6933, grad_fn=<MseLossBackward>)\n",
      "Epoch 20 Loss 9.3856878281:\n",
      "tensor(1.4497, grad_fn=<MseLossBackward>)\n",
      "tensor(6.0819, grad_fn=<MseLossBackward>)\n",
      "tensor(1.7473, grad_fn=<MseLossBackward>)\n",
      "Epoch 21 Loss 9.2789936066:\n",
      "tensor(1.1692, grad_fn=<MseLossBackward>)\n",
      "tensor(6.2707, grad_fn=<MseLossBackward>)\n",
      "tensor(1.7987, grad_fn=<MseLossBackward>)\n",
      "Epoch 22 Loss 9.2386035919:\n",
      "tensor(0.9609, grad_fn=<MseLossBackward>)\n",
      "tensor(6.4391, grad_fn=<MseLossBackward>)\n",
      "tensor(1.8447, grad_fn=<MseLossBackward>)\n",
      "Epoch 23 Loss 9.2446269989:\n",
      "tensor(0.8121, grad_fn=<MseLossBackward>)\n",
      "tensor(6.5800, grad_fn=<MseLossBackward>)\n",
      "tensor(1.8834, grad_fn=<MseLossBackward>)\n",
      "Epoch 24 Loss 9.2755355835:\n",
      "tensor(0.7102, grad_fn=<MseLossBackward>)\n",
      "tensor(6.6894, grad_fn=<MseLossBackward>)\n",
      "tensor(1.9139, grad_fn=<MseLossBackward>)\n",
      "Epoch 25 Loss 9.3134899139:\n",
      "tensor(0.6446, grad_fn=<MseLossBackward>)\n",
      "tensor(6.7658, grad_fn=<MseLossBackward>)\n",
      "tensor(1.9355, grad_fn=<MseLossBackward>)\n",
      "Epoch 26 Loss 9.3458604813:\n",
      "tensor(0.6074, grad_fn=<MseLossBackward>)\n",
      "tensor(6.8094, grad_fn=<MseLossBackward>)\n",
      "tensor(1.9482, grad_fn=<MseLossBackward>)\n",
      "Epoch 27 Loss 9.3649473190:\n",
      "tensor(0.5932, grad_fn=<MseLossBackward>)\n",
      "tensor(6.8219, grad_fn=<MseLossBackward>)\n",
      "tensor(1.9521, grad_fn=<MseLossBackward>)\n",
      "Epoch 28 Loss 9.3671131134:\n",
      "tensor(0.5985, grad_fn=<MseLossBackward>)\n",
      "tensor(6.8059, grad_fn=<MseLossBackward>)\n",
      "tensor(1.9475, grad_fn=<MseLossBackward>)\n",
      "Epoch 29 Loss 9.3518705368:\n",
      "tensor(0.6213, grad_fn=<MseLossBackward>)\n",
      "tensor(6.7648, grad_fn=<MseLossBackward>)\n",
      "tensor(1.9350, grad_fn=<MseLossBackward>)\n",
      "Epoch 30 Loss 9.3211259842:\n",
      "tensor(0.6607, grad_fn=<MseLossBackward>)\n",
      "tensor(6.7024, grad_fn=<MseLossBackward>)\n",
      "tensor(1.9155, grad_fn=<MseLossBackward>)\n",
      "Epoch 31 Loss 9.2785720825:\n",
      "tensor(0.7163, grad_fn=<MseLossBackward>)\n",
      "tensor(6.6230, grad_fn=<MseLossBackward>)\n",
      "tensor(1.8899, grad_fn=<MseLossBackward>)\n",
      "Epoch 32 Loss 9.2291173935:\n",
      "tensor(0.7879, grad_fn=<MseLossBackward>)\n",
      "tensor(6.5310, grad_fn=<MseLossBackward>)\n",
      "tensor(1.8595, grad_fn=<MseLossBackward>)\n",
      "Epoch 33 Loss 9.1782960892:\n",
      "tensor(0.8747, grad_fn=<MseLossBackward>)\n",
      "tensor(6.4312, grad_fn=<MseLossBackward>)\n",
      "tensor(1.8257, grad_fn=<MseLossBackward>)\n",
      "Epoch 34 Loss 9.1315774918:\n",
      "tensor(0.9747, grad_fn=<MseLossBackward>)\n",
      "tensor(6.3285, grad_fn=<MseLossBackward>)\n",
      "tensor(1.7903, grad_fn=<MseLossBackward>)\n",
      "Epoch 35 Loss 9.0935487747:\n",
      "tensor(1.0840, grad_fn=<MseLossBackward>)\n",
      "tensor(6.2281, grad_fn=<MseLossBackward>)\n",
      "tensor(1.7549, grad_fn=<MseLossBackward>)\n",
      "Epoch 36 Loss 9.0670700073:\n",
      "tensor(1.1962, grad_fn=<MseLossBackward>)\n",
      "tensor(6.1349, grad_fn=<MseLossBackward>)\n",
      "tensor(1.7216, grad_fn=<MseLossBackward>)\n",
      "Epoch 37 Loss 9.0526189804:\n",
      "tensor(1.3028, grad_fn=<MseLossBackward>)\n",
      "tensor(6.0535, grad_fn=<MseLossBackward>)\n",
      "tensor(1.6919, grad_fn=<MseLossBackward>)\n",
      "Epoch 38 Loss 9.0481500626:\n",
      "tensor(1.3944, grad_fn=<MseLossBackward>)\n",
      "tensor(5.9878, grad_fn=<MseLossBackward>)\n",
      "tensor(1.6674, grad_fn=<MseLossBackward>)\n",
      "Epoch 39 Loss 9.0496530533:\n",
      "tensor(1.4620, grad_fn=<MseLossBackward>)\n",
      "tensor(5.9410, grad_fn=<MseLossBackward>)\n",
      "tensor(1.6493, grad_fn=<MseLossBackward>)\n",
      "Epoch 40 Loss 9.0523185730:\n",
      "tensor(1.4991, grad_fn=<MseLossBackward>)\n",
      "tensor(5.9147, grad_fn=<MseLossBackward>)\n",
      "tensor(1.6381, grad_fn=<MseLossBackward>)\n",
      "Epoch 41 Loss 9.0518884659:\n",
      "tensor(1.5029, grad_fn=<MseLossBackward>)\n",
      "tensor(5.9089, grad_fn=<MseLossBackward>)\n",
      "tensor(1.6339, grad_fn=<MseLossBackward>)\n",
      "Epoch 42 Loss 9.0456571579:\n",
      "tensor(1.4750, grad_fn=<MseLossBackward>)\n",
      "tensor(5.9221, grad_fn=<MseLossBackward>)\n",
      "tensor(1.6358, grad_fn=<MseLossBackward>)\n",
      "Epoch 43 Loss 9.0329122543:\n",
      "tensor(1.4205, grad_fn=<MseLossBackward>)\n",
      "tensor(5.9515, grad_fn=<MseLossBackward>)\n",
      "tensor(1.6428, grad_fn=<MseLossBackward>)\n",
      "Epoch 44 Loss 9.0148220062:\n",
      "tensor(1.3476, grad_fn=<MseLossBackward>)\n",
      "tensor(5.9930, grad_fn=<MseLossBackward>)\n",
      "tensor(1.6534, grad_fn=<MseLossBackward>)\n",
      "Epoch 45 Loss 8.9939193726:\n",
      "tensor(1.2652, grad_fn=<MseLossBackward>)\n",
      "tensor(6.0421, grad_fn=<MseLossBackward>)\n",
      "tensor(1.6659, grad_fn=<MseLossBackward>)\n",
      "Epoch 46 Loss 8.9732036591:\n",
      "tensor(1.1821, grad_fn=<MseLossBackward>)\n",
      "tensor(6.0940, grad_fn=<MseLossBackward>)\n",
      "tensor(1.6790, grad_fn=<MseLossBackward>)\n",
      "Epoch 47 Loss 8.9551954269:\n",
      "tensor(1.1054, grad_fn=<MseLossBackward>)\n",
      "tensor(6.1444, grad_fn=<MseLossBackward>)\n",
      "tensor(1.6914, grad_fn=<MseLossBackward>)\n",
      "Epoch 48 Loss 8.9412517548:\n",
      "tensor(1.0400, grad_fn=<MseLossBackward>)\n",
      "tensor(6.1893, grad_fn=<MseLossBackward>)\n",
      "tensor(1.7020, grad_fn=<MseLossBackward>)\n",
      "Epoch 49 Loss 8.9314002991:\n",
      "tensor(0.9886, grad_fn=<MseLossBackward>)\n",
      "tensor(6.2259, grad_fn=<MseLossBackward>)\n",
      "tensor(1.7101, grad_fn=<MseLossBackward>)\n",
      "Epoch 50 Loss 8.9246358871:\n",
      "tensor(0.9521, grad_fn=<MseLossBackward>)\n",
      "tensor(6.2522, grad_fn=<MseLossBackward>)\n",
      "tensor(1.7151, grad_fn=<MseLossBackward>)\n",
      "Epoch 51 Loss 8.9194583893:\n",
      "tensor(0.9303, grad_fn=<MseLossBackward>)\n",
      "tensor(6.2672, grad_fn=<MseLossBackward>)\n",
      "tensor(1.7169, grad_fn=<MseLossBackward>)\n",
      "Epoch 52 Loss 8.9143753052:\n",
      "tensor(0.9221, grad_fn=<MseLossBackward>)\n",
      "tensor(6.2710, grad_fn=<MseLossBackward>)\n",
      "tensor(1.7152, grad_fn=<MseLossBackward>)\n",
      "Epoch 53 Loss 8.9082527161:\n",
      "tensor(0.9258, grad_fn=<MseLossBackward>)\n",
      "tensor(6.2643, grad_fn=<MseLossBackward>)\n",
      "tensor(1.7104, grad_fn=<MseLossBackward>)\n",
      "Epoch 54 Loss 8.9004755020:\n",
      "tensor(0.9396, grad_fn=<MseLossBackward>)\n",
      "tensor(6.2486, grad_fn=<MseLossBackward>)\n",
      "tensor(1.7027, grad_fn=<MseLossBackward>)\n",
      "Epoch 55 Loss 8.8909463882:\n",
      "tensor(0.9613, grad_fn=<MseLossBackward>)\n",
      "tensor(6.2260, grad_fn=<MseLossBackward>)\n",
      "tensor(1.6928, grad_fn=<MseLossBackward>)\n",
      "Epoch 56 Loss 8.8800058365:\n",
      "tensor(0.9885, grad_fn=<MseLossBackward>)\n",
      "tensor(6.1986, grad_fn=<MseLossBackward>)\n",
      "tensor(1.6812, grad_fn=<MseLossBackward>)\n",
      "Epoch 57 Loss 8.8682985306:\n",
      "tensor(1.0191, grad_fn=<MseLossBackward>)\n",
      "tensor(6.1689, grad_fn=<MseLossBackward>)\n",
      "tensor(1.6686, grad_fn=<MseLossBackward>)\n",
      "Epoch 58 Loss 8.8566312790:\n",
      "tensor(1.0508, grad_fn=<MseLossBackward>)\n",
      "tensor(6.1390, grad_fn=<MseLossBackward>)\n",
      "tensor(1.6560, grad_fn=<MseLossBackward>)\n",
      "Epoch 59 Loss 8.8457813263:\n",
      "tensor(1.0815, grad_fn=<MseLossBackward>)\n",
      "tensor(6.1109, grad_fn=<MseLossBackward>)\n",
      "tensor(1.6439, grad_fn=<MseLossBackward>)\n",
      "Epoch 60 Loss 8.8363285065:\n",
      "tensor(1.1090, grad_fn=<MseLossBackward>)\n",
      "tensor(6.0865, grad_fn=<MseLossBackward>)\n",
      "tensor(1.6330, grad_fn=<MseLossBackward>)\n",
      "Epoch 61 Loss 8.8284902573:\n",
      "tensor(1.1315, grad_fn=<MseLossBackward>)\n",
      "tensor(6.0667, grad_fn=<MseLossBackward>)\n",
      "tensor(1.6239, grad_fn=<MseLossBackward>)\n",
      "Epoch 62 Loss 8.8220443726:\n",
      "tensor(1.1473, grad_fn=<MseLossBackward>)\n",
      "tensor(6.0525, grad_fn=<MseLossBackward>)\n",
      "tensor(1.6166, grad_fn=<MseLossBackward>)\n",
      "Epoch 63 Loss 8.8163986206:\n",
      "tensor(1.1554, grad_fn=<MseLossBackward>)\n",
      "tensor(6.0438, grad_fn=<MseLossBackward>)\n",
      "tensor(1.6115, grad_fn=<MseLossBackward>)\n",
      "Epoch 64 Loss 8.8107967377:\n",
      "tensor(1.1555, grad_fn=<MseLossBackward>)\n",
      "tensor(6.0407, grad_fn=<MseLossBackward>)\n",
      "tensor(1.6084, grad_fn=<MseLossBackward>)\n",
      "Epoch 65 Loss 8.8045864105:\n",
      "tensor(1.1479, grad_fn=<MseLossBackward>)\n",
      "tensor(6.0425, grad_fn=<MseLossBackward>)\n",
      "tensor(1.6071, grad_fn=<MseLossBackward>)\n",
      "Epoch 66 Loss 8.7974567413:\n",
      "tensor(1.1340, grad_fn=<MseLossBackward>)\n",
      "tensor(6.0483, grad_fn=<MseLossBackward>)\n",
      "tensor(1.6072, grad_fn=<MseLossBackward>)\n",
      "Epoch 67 Loss 8.7895174026:\n",
      "tensor(1.1156, grad_fn=<MseLossBackward>)\n",
      "tensor(6.0572, grad_fn=<MseLossBackward>)\n",
      "tensor(1.6085, grad_fn=<MseLossBackward>)\n",
      "Epoch 68 Loss 8.7812185287:\n",
      "tensor(1.0947, grad_fn=<MseLossBackward>)\n",
      "tensor(6.0680, grad_fn=<MseLossBackward>)\n",
      "tensor(1.6105, grad_fn=<MseLossBackward>)\n",
      "Epoch 69 Loss 8.7731342316:\n",
      "tensor(1.0732, grad_fn=<MseLossBackward>)\n",
      "tensor(6.0798, grad_fn=<MseLossBackward>)\n",
      "tensor(1.6127, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-ffda5e6527f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpinn_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_b\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minterpolation_criterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboundary_criterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpde_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch %d Loss %.10f:\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    loss = model.pinn_loss(x, u_0, u_b,interpolation_criterion=nn.MSELoss(), boundary_criterion=nn.MSELoss(), pde_norm=nn.MSELoss())\n",
    "    loss.backward()\n",
    "    print(\"Epoch %d Loss %.10f:\"%(epoch + 1, loss.item()))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
